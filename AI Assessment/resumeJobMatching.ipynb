{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Resume Job Description Matching"
      ],
      "metadata": {
        "id": "xE1tdIWPs7wF"
      },
      "id": "xE1tdIWPs7wF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "OB7BWs9YsqOP"
      },
      "id": "OB7BWs9YsqOP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "kw2MNLQobtLH",
        "outputId": "ed3e1056-06aa-47ff-cff9-1a82e9b6d3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kw2MNLQobtLH",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "eqmwP9O_syuq"
      },
      "id": "eqmwP9O_syuq"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4e35b23b-2b1f-49f0-b77a-67f6023fed73",
      "metadata": {
        "id": "4e35b23b-2b1f-49f0-b77a-67f6023fed73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "d5becb0f-8800-4553-b9b6-8c5336db60b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-40cacdc85244>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# library imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset and shuffle the data in it"
      ],
      "metadata": {
        "id": "B-vv_mIAtEAj"
      },
      "id": "B-vv_mIAtEAj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset which is on my github has the three classes (Good Fit, Potential Fit, No Fit) grouped together. This will lead to class imbalance in the training data which leads to poor predictive results. To handle this, shuffling has been done using the `.sample()` pandas method for better generalization."
      ],
      "metadata": {
        "id": "cLp8G2A3tjf3"
      },
      "id": "cLp8G2A3tjf3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2251e2f7-6000-47bd-b2eb-199d6418b3d8",
      "metadata": {
        "id": "2251e2f7-6000-47bd-b2eb-199d6418b3d8"
      },
      "outputs": [],
      "source": [
        "resume_match_df = pd.read_csv(\"https://raw.githubusercontent.com/samariwa/artificial-intelligence-projects/refs/heads/main/AI%20Assessment/data.csv\").sample(frac=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization of text"
      ],
      "metadata": {
        "id": "dW5mL77fu8l3"
      },
      "id": "dW5mL77fu8l3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize SpaCy's English model that can be used to convert the resume and job description text into tokens that can be used to perform classification using various techniques. This involves making all characters lowercase, lemmatizing the text and removing stop words (which are of no significance in the classification)."
      ],
      "metadata": {
        "id": "ZfxJgfzbvITN"
      },
      "id": "ZfxJgfzbvITN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c28eb7-ea59-4a08-902f-f8066217c974",
      "metadata": {
        "id": "48c28eb7-ea59-4a08-902f-f8066217c974"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24425f2a-932a-4d83-a002-60e271eda3bf",
      "metadata": {
        "id": "24425f2a-932a-4d83-a002-60e271eda3bf"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    tokenize_text(text)\n",
        "    This function will tokenize the texts from the resumes and job descriptions\n",
        "    This will be first preprocessed by removing stop words that add no semantic value\n",
        "    and lemmatized for a standardized set of words\n",
        "    text(string): the text to be tokenized\n",
        "    returns: list of tokens generated from the text passed in as an arg\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform the tokenization for each resume and each job description and store the list of tokens in their respective lists"
      ],
      "metadata": {
        "id": "6va5L4Ahwaay"
      },
      "id": "6va5L4Ahwaay"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ea01f4-6c24-4416-8a30-38cbf8ecafb7",
      "metadata": {
        "id": "c4ea01f4-6c24-4416-8a30-38cbf8ecafb7"
      },
      "outputs": [],
      "source": [
        "# initialize string that will contain tokens for each resume and job description\n",
        "preprocessed_resume_text = []\n",
        "preprocessed_jd_text = []\n",
        "for i in tqdm(range(len(resume_match_df))):\n",
        "    preprocessed_resume_text.append(tokenize_text(resume_match_df.iloc[i]['resume_text']))\n",
        "    preprocessed_jd_text.append(tokenize_text(resume_match_df.iloc[i]['job_description_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each resume and JD (job desription, rejoin the tokens into strings using the space \" \" delimiter.\n",
        "\n",
        "Join the sets of words, resume_text and jd_text into one pool of words which woll be processed by the various vectorizers accordingly."
      ],
      "metadata": {
        "id": "zMLAua0-xFu8"
      },
      "id": "zMLAua0-xFu8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be146ea2-54ca-490f-923f-c9130fe33f9c",
      "metadata": {
        "id": "be146ea2-54ca-490f-923f-c9130fe33f9c"
      },
      "outputs": [],
      "source": [
        "# convert the tokens in each list back to strings\n",
        "resume_texts = [\" \".join(tokens) for tokens in tqdm(preprocessed_resume_text)]\n",
        "jd_texts = [\" \".join(tokens) for tokens in tqdm(preprocessed_jd_text)]\n",
        "'''\n",
        "combine the resume and job description texts before count vectorizing\n",
        "In each row we represent the tokens as values counts for the number of\n",
        "times they appear in the document\n",
        "'''\n",
        "\n",
        "corpus = resume_texts + jd_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c54a788-16c3-4d59-803c-7588163a9158",
      "metadata": {
        "id": "5c54a788-16c3-4d59-803c-7588163a9158"
      },
      "source": [
        "## Vectorization using the Bag of Words Vectorizer\n",
        "\n",
        "1.   Initialize the BoW count vectorizer\n",
        "2.   Transform the words into vectors which are represented by the number of times they appear in a sentence (the joined tokens)\n",
        "3. Reseparate the resume and jd text and put them side-by-side (a resume with is correpsonding JD in one row). This will act as the features and the outcomes of the selection process as the labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a68836-6e3d-437a-a723-fe3ad8b256b2",
      "metadata": {
        "id": "26a68836-6e3d-437a-a723-fe3ad8b256b2"
      },
      "outputs": [],
      "source": [
        "# initialize bag of words vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# transform the corpus (resume + jd text) into vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "# separate the resume and jd text and set them side-by-side\n",
        "resume_bow = bow_matrix[:len(resume_texts)].toarray()\n",
        "jd_bow = bow_matrix[len(resume_texts):].toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e198cf-6bfd-4433-939a-f0756449b063",
      "metadata": {
        "id": "66e198cf-6bfd-4433-939a-f0756449b063"
      },
      "outputs": [],
      "source": [
        "# we horizontally stack JDs and resumes so that we have numeric representations od words for JDs and corresponding resume\n",
        "X = np.hstack((resume_bow, jd_bow))\n",
        "# this representation created above will be set against the job-match decision\n",
        "y = resume_match_df['label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba4f239-a5fa-4e56-a16d-eb36ddcd3eb8",
      "metadata": {
        "id": "1ba4f239-a5fa-4e56-a16d-eb36ddcd3eb8"
      },
      "source": [
        "### Run classification of the data using various algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First it is necessary to establish which algorithm performs well for this problem. This can iteratively be done through cross-validation using several algorithms and this can be helpful is establishing how well generalized our data is."
      ],
      "metadata": {
        "id": "LSnri9pJ1VKF"
      },
      "id": "LSnri9pJ1VKF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b69922-fda7-46ef-ad9c-2beb3bfb5905",
      "metadata": {
        "id": "39b69922-fda7-46ef-ad9c-2beb3bfb5905"
      },
      "outputs": [],
      "source": [
        "def classify_k_fold_x_validation(model, x_data, y_data, folds):\n",
        "    \"\"\"\n",
        "    classify_k_fold_x_validation(model, x_data, y_data, folds)\n",
        "    This function performs k-fold cross validation by running trainings and testing on\n",
        "    different segments of the dataset. The number of segments id defined as an argument\n",
        "    model: The model being used for cross-validation\n",
        "    x_data: the set of features in the data\n",
        "    y_data: the labels of the data\n",
        "    folds: The value of k representing the number of divisions of the data to be done for cross-validation\n",
        "    \"\"\"\n",
        "    # list to store accuracy for each model\n",
        "    accuracy_scores = []\n",
        "    classifier = model()\n",
        "    k_fold = KFold(n_splits=folds, shuffle=False)\n",
        "    # For each fold(split) run classification and get the accuracy metric\n",
        "    for train_idx, test_idx in k_fold.split(x_data):\n",
        "        X_train, X_test, y_train, y_test = x_data[train_idx], x_data[test_idx], y_data[train_idx], y_data[test_idx]\n",
        "        classifier.fit(X_train, y_train)\n",
        "        accuracy_scores.append(classifier.score(X_test, y_test))\n",
        "\n",
        "    return accuracy_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4c5c30-d453-433b-bf66-4a38c2bf9c09",
      "metadata": {
        "id": "4a4c5c30-d453-433b-bf66-4a38c2bf9c09"
      },
      "outputs": [],
      "source": [
        "def model_validation(x_data, y_data, folds=10):\n",
        "    \"\"\"\n",
        "    model_validation(x_data, y_data, folds=10)\n",
        "    Validates performance of various models\n",
        "    It passes each algorithm specified in the `models` list\n",
        "    to the classify_k_fold_x_validation(model, x_data, y_data, folds)\n",
        "    function together with feature and labels and the number of folds\n",
        "    expected.\n",
        "    \"\"\"\n",
        "    models = {'K-Nearest Neighbors': 'KNeighborsClassifier',\n",
        "              'Decision Tree': 'DecisionTreeClassifier',\n",
        "              'Random Forest': 'RandomForestClassifier',\n",
        "              'Support Vector Classifier': 'SVC',\n",
        "              'Gaussian Naive Bayes': 'GaussianNB'}\n",
        "    model_performance = {}\n",
        "    # For each model run cross-validation\n",
        "    for model_name, model_function in tqdm(models.items()):\n",
        "        accuracies = classify_k_fold_x_validation(eval(model_function), x_data, y_data, folds)\n",
        "        model_performance[model_name] = accuracies\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Visualize each model's accuracy in a box and whisker plot\n",
        "    plt.boxplot(list(model_performance.values()), labels=list(model_performance.keys()))\n",
        "    plt.title('Classification Performance Comparison with K-Fold Cross Validation')\n",
        "    plt.xlabel('Classifier Model')\n",
        "    plt.ylabel('Accuracy Scores')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ebd99f-9f9b-4d60-bf18-f20f74229760",
      "metadata": {
        "id": "45ebd99f-9f9b-4d60-bf18-f20f74229760"
      },
      "outputs": [],
      "source": [
        "model_validation(X, y, folds=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the model that performs best, run the model and perform hyper-parameter tuning"
      ],
      "metadata": {
        "id": "POKY3jT34YXv"
      },
      "id": "POKY3jT34YXv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2292a75a-38a6-4539-839e-055ac1ac748c",
      "metadata": {
        "id": "2292a75a-38a6-4539-839e-055ac1ac748c"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization using TF-IDF"
      ],
      "metadata": {
        "id": "oQsuDNFVNwyY"
      },
      "id": "oQsuDNFVNwyY"
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "resume_tfidf = tfidf_vectorizer.fit_transform(resume_texts)\n",
        "jd_tfidf = tfidf_vectorizer.fit_transform(jd_texts)"
      ],
      "metadata": {
        "id": "BA0ueOkjNz1x"
      },
      "id": "BA0ueOkjNz1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = hstack((resume_tfidf, jd_tfidf)).toarray()\n",
        "y = resume_match_df['label'].values"
      ],
      "metadata": {
        "id": "40tzRpt2O2Gu"
      },
      "id": "40tzRpt2O2Gu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_validation(X, y, folds=10)"
      ],
      "metadata": {
        "id": "FBLIdbkEPAf1"
      },
      "id": "FBLIdbkEPAf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "K_bTK16uO4yl"
      },
      "id": "K_bTK16uO4yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Cosine Similarity Evaluation"
      ],
      "metadata": {
        "id": "VGdR-_6g79AZ"
      },
      "id": "VGdR-_6g79AZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We combine resume and JD text together because cosine similarity compares vectors of the same dimension"
      ],
      "metadata": {
        "id": "kCxHgsxR-1fN"
      },
      "id": "kCxHgsxR-1fN"
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# split back to resume and jd matrices\n",
        "cos_sim_resume_tfidf = tfidf_matrix[:len(resume_texts)]\n",
        "cos_sim_jd_tfidf = tfidf_matrix[len(resume_texts):]\n",
        "\n",
        "similarity_scores = [cosine_similarity(cos_sim_resume_tfidf[i], cos_sim_jd_tfidf[i])[0][0] for i in range(len(resume_tfidf.toarray()))]"
      ],
      "metadata": {
        "id": "IQNNKGRc8Bfn"
      },
      "id": "IQNNKGRc8Bfn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_similarity_scores(scores):\n",
        "  similarity_scores = []\n",
        "  for score in scores:\n",
        "    if score >= 0.2:\n",
        "      similarity_scores.append(\"Good Fit\")\n",
        "    elif score >= 0.1 and score < 0.2:\n",
        "      similarity_scores.append(\"Potential Fit\")\n",
        "    elif score < 0.1:\n",
        "      similarity_scores.append(\"No Fit\")\n",
        "\n",
        "  predictions = pd.DataFrame(similarity_scores)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "cos_sim_pred = classify_similarity_scores(similarity_scores)"
      ],
      "metadata": {
        "id": "2AojnVrUCz_W"
      },
      "id": "2AojnVrUCz_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "accuracy = accuracy_score(y, cos_sim_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "UqZtPrmlGb0v"
      },
      "id": "UqZtPrmlGb0v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_df = pd.DataFrame(y)\n",
        "no_fit_actual = y_df.loc[y_df[0] == \"No Fit\"].count()[0]\n",
        "pot_fit_actual = y_df.loc[y_df[0] == \"Potential Fit\"].count()[0]\n",
        "good_fit_actual = y_df.loc[y_df[0] == \"Good Fit\"].count()[0]\n",
        "no_fit_pred = cos_sim_pred.loc[cos_sim_pred[0] == \"No Fit\"].count()[0]\n",
        "pot_fit_pred = cos_sim_pred.loc[cos_sim_pred[0] == \"Potential Fit\"].count()[0]\n",
        "good_fit_pred = cos_sim_pred.loc[cos_sim_pred[0] == \"Good Fit\"].count()[0]\n",
        "print(f\"Good Fit Predictions Count:{good_fit_pred}...Good Fit Actual Count:{good_fit_actual}\")\n",
        "print(f\"Potential Fit Predictions Count:{pot_fit_pred}...Potential Fit Actual Count:{pot_fit_actual}\")\n",
        "print(f\"Not Fit Predictions Count:{no_fit_pred}...Not Fit Actual Count:{no_fit_actual}\")"
      ],
      "metadata": {
        "id": "86BmT8kae3UT"
      },
      "id": "86BmT8kae3UT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization using word embeddings (Word2Vec)"
      ],
      "metadata": {
        "id": "oouMdJ-MUZYF"
      },
      "id": "oouMdJ-MUZYF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine preprocessed resume and JD tokens for training Word2Vec\n",
        "all_tokens = preprocessed_resume_text + preprocessed_jd_text\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to get the average Word2Vec vector for a document\n",
        "def get_vector(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else model.vector_size\n",
        "\n",
        "# Convert resumes & JDs into numerical vectors\n",
        "resume_vectors = np.array([get_vector(tokens, word2vec_model) for tokens in preprocessed_resume_text])\n",
        "jd_vectors = np.array([get_vector(tokens, word2vec_model) for tokens in preprocessed_jd_text])\n",
        "\n",
        "X = np.hstack((resume_vectors, jd_vectors))\n",
        "y = resume_match_df['label'].values"
      ],
      "metadata": {
        "id": "4kRw15uyUeMt"
      },
      "id": "4kRw15uyUeMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_validation(X, y, folds=10)"
      ],
      "metadata": {
        "id": "5kBfFU05VF21"
      },
      "id": "5kBfFU05VF21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification using the best performing classifier"
      ],
      "metadata": {
        "id": "lIvw9mNwYVnA"
      },
      "id": "lIvw9mNwYVnA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "WSptKqp4VE61"
      },
      "id": "WSptKqp4VE61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec Cosine Similarity Evaluation"
      ],
      "metadata": {
        "id": "82qm_DjYYoLx"
      },
      "id": "82qm_DjYYoLx"
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_scores_w2v = [cosine_similarity([cos_sim_resume_w2v], [cos_sim_jd_w2v])[0][0] for cos_sim_resume_w2v, cos_sim_jd_w2v in zip(resume_vectors, jd_vectors)]"
      ],
      "metadata": {
        "id": "el9DQ-_nVMmL"
      },
      "id": "el9DQ-_nVMmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim_pred_w2v = classify_similarity_scores(similarity_scores_w2v)"
      ],
      "metadata": {
        "id": "MLK5F-ABdSaN"
      },
      "id": "MLK5F-ABdSaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "accuracy = accuracy_score(y, cos_sim_pred_w2v)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "K1XXi7fMdjGk"
      },
      "id": "K1XXi7fMdjGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Neural Network with word embeddings (Word2Vec & PyTorch)"
      ],
      "metadata": {
        "id": "_esNKA6LH-cj"
      },
      "id": "_esNKA6LH-cj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine resume and JD vectors into single vectors\n",
        "X = np.concatenate([resume_vectors, jd_vectors], axis=1)\n",
        "\n",
        "# Encode labels and convert the features and labels to tensors\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4FkAQ0bOH9L3"
      },
      "id": "4FkAQ0bOH9L3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dimension = X.shape[1]\n",
        "hidden_layers = 128\n",
        "output_dimension = len(np.unique(y_encoded))\n",
        "\n",
        "# Hyperparameter initialization\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Du8QZshrKJnq"
      },
      "id": "Du8QZshrKJnq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly initialize weights and biases\n",
        "W1 = torch.randn(input_dimension, hidden_layers, requires_grad=True)\n",
        "W1.data *= 0.01\n",
        "\n",
        "b1 = torch.zeros(hidden_layers, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn(hidden_layers, output_dimension, requires_grad=True)\n",
        "W2.data *= 0.01\n",
        "\n",
        "b2 = torch.zeros(output_dimension, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.Adam([W1, b1, W2, b2], lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "0kWf0PQWKKuS"
      },
      "id": "0kWf0PQWKKuS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    permutation = torch.randperm(X_train.size(0))  # shuffle\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        z1 = batch_x @ W1 + b1  # Linear\n",
        "        a1 = F.relu(z1)         # Activation\n",
        "        z2 = a1 @ W2 + b2       # Output layer\n",
        "        loss = criterion(z2, batch_y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "GDRNuoRYK5V-"
      },
      "id": "GDRNuoRYK5V-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    z1 = X_test @ W1 + b1\n",
        "    a1 = F.relu(z1)\n",
        "    z2 = a1 @ W2 + b2\n",
        "    predictions = torch.argmax(z2, dim=1)\n",
        "\n",
        "    accuracy = (predictions == y_test).float().mean()\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "-Cjune9NK8mk"
      },
      "id": "-Cjune9NK8mk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using DISTILBERT Pretrained model"
      ],
      "metadata": {
        "id": "jNntLpEzeXdh"
      },
      "id": "jNntLpEzeXdh"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "z8NJYWFImUQz"
      },
      "id": "z8NJYWFImUQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "UoyC1eHjmYlr"
      },
      "id": "UoyC1eHjmYlr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label to numeric\n",
        "resume_match_df['label'] = resume_match_df['label'].map({'No Fit': 0, 'Potential Fit': 1, 'Good Fit': 2})\n",
        "\n",
        "# Convert into list of sentence pairs\n",
        "texts = list(zip(resume_match_df['resume_text'], resume_match_df['job_description_text']))\n",
        "labels = resume_match_df['label'].tolist()"
      ],
      "metadata": {
        "id": "pTnhJceIYphZ"
      },
      "id": "pTnhJceIYphZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', num_labels=3)"
      ],
      "metadata": {
        "id": "m_FrHdOves1r"
      },
      "id": "m_FrHdOves1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(texts, labels):\n",
        "    encodings = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = encodings['input_ids']\n",
        "    attention_mask = encodings['attention_mask']\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return TensorDataset(input_ids, attention_mask, labels_tensor)"
      ],
      "metadata": {
        "id": "0tsseYk8fZPc"
      },
      "id": "0tsseYk8fZPc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "train_dataset = tokenize(X_train, y_train)\n",
        "test_dataset = tokenize(X_test, y_test)"
      ],
      "metadata": {
        "id": "xjRE0h-jfgtR"
      },
      "id": "xjRE0h-jfgtR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)"
      ],
      "metadata": {
        "id": "iiIbzIdMiNe9"
      },
      "id": "iiIbzIdMiNe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QMbJFnQMjVmb"
      },
      "id": "QMbJFnQMjVmb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 3  # 3 epochs\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "FQzPqUl9joAE"
      },
      "id": "FQzPqUl9joAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    model.train()\n",
        "    for epoch in range(15):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "        loop = tqdm(train_loader)\n",
        "        for batch in loop:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loop.set_description(f\"Epoch {epoch+1}\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "train_model()"
      ],
      "metadata": {
        "id": "kHDmcOIjj6r0"
      },
      "id": "kHDmcOIjj6r0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    target_names = ['No Fit', 'Potential Fit', 'Good Fit']\n",
        "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
        "\n",
        "evaluate_model()\n"
      ],
      "metadata": {
        "id": "B2doBcvHkVRR"
      },
      "id": "B2doBcvHkVRR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}