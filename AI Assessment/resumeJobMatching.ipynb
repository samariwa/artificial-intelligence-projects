{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -U\n",
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "id": "kw2MNLQobtLH",
        "outputId": "c6f0efc3-8a1d-43ba-97ee-691a0a5bbc57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kw2MNLQobtLH",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4e35b23b-2b1f-49f0-b77a-67f6023fed73",
      "metadata": {
        "id": "4e35b23b-2b1f-49f0-b77a-67f6023fed73",
        "outputId": "1360166d-a701-4a0f-ad66-af41be34c50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-96774ce69b48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2251e2f7-6000-47bd-b2eb-199d6418b3d8",
      "metadata": {
        "id": "2251e2f7-6000-47bd-b2eb-199d6418b3d8"
      },
      "outputs": [],
      "source": [
        "resume_match_df = pd.read_csv(\"https://raw.githubusercontent.com/samariwa/artificial-intelligence-projects/refs/heads/main/AI%20Assessment/data.csv\").head(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c28eb7-ea59-4a08-902f-f8066217c974",
      "metadata": {
        "id": "48c28eb7-ea59-4a08-902f-f8066217c974"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24425f2a-932a-4d83-a002-60e271eda3bf",
      "metadata": {
        "id": "24425f2a-932a-4d83-a002-60e271eda3bf"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    tokenize_text(text)\n",
        "    This function will tokenize the texts from the resumes and job descriptions\n",
        "    This will be first preprocessed by removing stop words that add no semantic value\n",
        "    and lemmatized for a standardized set of words\n",
        "    text(string): the text to be tokenized\n",
        "    returns: list of tokens generated from the text passed in as an arg\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ea01f4-6c24-4416-8a30-38cbf8ecafb7",
      "metadata": {
        "id": "c4ea01f4-6c24-4416-8a30-38cbf8ecafb7"
      },
      "outputs": [],
      "source": [
        "# initialize string that will contain tokens for each resume and job description\n",
        "preprocessed_resume_text = []\n",
        "preprocessed_jd_text = []\n",
        "for i in tqdm(range(len(resume_match_df))):\n",
        "    preprocessed_resume_text.append(tokenize_text(resume_match_df.iloc[i]['resume_text']))\n",
        "    preprocessed_jd_text.append(tokenize_text(resume_match_df.iloc[i]['job_description_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be146ea2-54ca-490f-923f-c9130fe33f9c",
      "metadata": {
        "id": "be146ea2-54ca-490f-923f-c9130fe33f9c"
      },
      "outputs": [],
      "source": [
        "# convert the tokens in each list back to strings\n",
        "resume_texts = [\" \".join(tokens) for tokens in tqdm(preprocessed_resume_text)]\n",
        "jd_texts = [\" \".join(tokens) for tokens in tqdm(preprocessed_jd_text)]\n",
        "'''\n",
        "combine the resume and job description texts before count vectorizing\n",
        "In each row we represent the tokens as values counts for the number of\n",
        "times they appear in the document\n",
        "'''\n",
        "corpus = resume_texts + jd_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c54a788-16c3-4d59-803c-7588163a9158",
      "metadata": {
        "id": "5c54a788-16c3-4d59-803c-7588163a9158"
      },
      "source": [
        "## Vectorization using the Bag of Words Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a68836-6e3d-437a-a723-fe3ad8b256b2",
      "metadata": {
        "id": "26a68836-6e3d-437a-a723-fe3ad8b256b2"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "resume_bow = bow_matrix[:len(resume_texts)].toarray()\n",
        "jd_bow = bow_matrix[len(resume_texts):].toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e198cf-6bfd-4433-939a-f0756449b063",
      "metadata": {
        "id": "66e198cf-6bfd-4433-939a-f0756449b063"
      },
      "outputs": [],
      "source": [
        "# we horizontally stack JDs and resumes so that we have numeric representations od words for JDs and corresponding resume\n",
        "X = np.hstack((resume_bow, jd_bow))\n",
        "# this representation created above will be set against the job-match decision\n",
        "y = resume_match_df['label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba4f239-a5fa-4e56-a16d-eb36ddcd3eb8",
      "metadata": {
        "id": "1ba4f239-a5fa-4e56-a16d-eb36ddcd3eb8"
      },
      "source": [
        "### Run classification of the data using various algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b69922-fda7-46ef-ad9c-2beb3bfb5905",
      "metadata": {
        "id": "39b69922-fda7-46ef-ad9c-2beb3bfb5905"
      },
      "outputs": [],
      "source": [
        "def classify_k_fold_x_validation(model, x_data, y_data, folds):\n",
        "    \"\"\"\n",
        "    classify_k_fold_x_validation(model, x_data, y_data, folds)\n",
        "    \"\"\"\n",
        "    accuracy_scores = []\n",
        "    classifier = model()\n",
        "    k_fold = KFold(n_splits=folds, shuffle=False)\n",
        "    for train_idx, test_idx in k_fold.split(x_data):\n",
        "        X_train, X_test, y_train, y_test = x_data[train_idx], x_data[test_idx], y_data[train_idx], y_data[test_idx]\n",
        "        classifier.fit(X_train, y_train)\n",
        "        accuracy_scores.append(classifier.score(X_test, y_test))\n",
        "\n",
        "    return accuracy_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4c5c30-d453-433b-bf66-4a38c2bf9c09",
      "metadata": {
        "id": "4a4c5c30-d453-433b-bf66-4a38c2bf9c09"
      },
      "outputs": [],
      "source": [
        "def model_validation(x_data, y_data, folds=10):\n",
        "    \"\"\"\n",
        "    model_validation(x_data, y_data, folds=10)\n",
        "    Validates performance of various models\n",
        "    \"\"\"\n",
        "    models = {'K-Nearest Neighbors': 'KNeighborsClassifier',\n",
        "              'Decision Tree': 'DecisionTreeClassifier',\n",
        "              'Random Forest': 'RandomForestClassifier',\n",
        "              'Support Vector Classifier': 'SVC',\n",
        "              'Gaussian Naive Bayes': 'GaussianNB'}\n",
        "    model_performance = {}\n",
        "    for model_name, model_function in models.items():\n",
        "        accuracies = classify_k_fold_x_validation(eval(model_function), x_data, y_data, folds)\n",
        "        model_performance[model_name] = accuracies\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.boxplot(list(model_performance.values()), labels=list(model_performance.keys()))\n",
        "    plt.title('Classification Performance Comparison with K-Fold Cross Validation')\n",
        "    plt.xlabel('Classifier Model')\n",
        "    plt.ylabel('Accuracy Scores')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ebd99f-9f9b-4d60-bf18-f20f74229760",
      "metadata": {
        "id": "45ebd99f-9f9b-4d60-bf18-f20f74229760"
      },
      "outputs": [],
      "source": [
        "model_validation(X, y, folds=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2292a75a-38a6-4539-839e-055ac1ac748c",
      "metadata": {
        "id": "2292a75a-38a6-4539-839e-055ac1ac748c"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization using TF-IDF"
      ],
      "metadata": {
        "id": "oQsuDNFVNwyY"
      },
      "id": "oQsuDNFVNwyY"
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "resume_tfidf = tfidf_vectorizer.fit_transform(resume_texts)\n",
        "jd_tfidf = tfidf_vectorizer.fit_transform(jd_texts)"
      ],
      "metadata": {
        "id": "BA0ueOkjNz1x"
      },
      "id": "BA0ueOkjNz1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.hstack((resume_tfidf, jd_tfidf))\n",
        "y = resume_match_df['label'].values"
      ],
      "metadata": {
        "id": "40tzRpt2O2Gu"
      },
      "id": "40tzRpt2O2Gu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_validation(X, y, folds=10)"
      ],
      "metadata": {
        "id": "FBLIdbkEPAf1"
      },
      "id": "FBLIdbkEPAf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "K_bTK16uO4yl"
      },
      "id": "K_bTK16uO4yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization using Word2Vec"
      ],
      "metadata": {
        "id": "oouMdJ-MUZYF"
      },
      "id": "oouMdJ-MUZYF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine preprocessed resume and JD tokens for training Word2Vec\n",
        "all_tokens = preprocessed_resume_text + preprocessed_jd_text\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to get the average Word2Vec vector for a document\n",
        "def get_vector(tokens, model, vector_size=100):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
        "\n",
        "# Convert resumes & JDs into numerical vectors\n",
        "resume_vectors = np.array([get_vector(tokens, word2vec_model) for tokens in preprocessed_resume_text])\n",
        "jd_vectors = np.array([get_vector(tokens, word2vec_model) for tokens in preprocessed_jd_text])\n",
        "\n",
        "X = np.hstack((resume_vectors, jd_vectors))\n",
        "y = resume_match_df['label'].values"
      ],
      "metadata": {
        "id": "4kRw15uyUeMt"
      },
      "id": "4kRw15uyUeMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_validation(X, y, folds=10)"
      ],
      "metadata": {
        "id": "5kBfFU05VF21"
      },
      "id": "5kBfFU05VF21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "WSptKqp4VE61"
      },
      "id": "WSptKqp4VE61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "el9DQ-_nVMmL"
      },
      "id": "el9DQ-_nVMmL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}