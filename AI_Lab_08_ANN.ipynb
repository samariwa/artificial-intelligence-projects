{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v_eYp43AJsq"
      },
      "source": [
        "# AI Lab 08\n",
        "\n",
        "Data Transformation and MLPs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement:**\n",
        "Classification of Wines Using Machine Learning\n",
        "Wine classification is a crucial task in the food and beverage industry, enabling the differentiation of wines based on their chemical composition. The Wine dataset from the UCI Machine Learning Repository\n",
        "\n",
        "    https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
        "\n",
        "provides a structured collection of 178 wine samples, each described by 13 numerical features, including alcohol content, magnesium levels, and color intensity. These samples belong to three distinct classes, representing different types of wine grown in the same region of Italy.\n",
        "\n",
        "The goal of this study is to develop a machine learning model, particularly artificial neural network model that can accurately classify wines based on their chemical attributes.\n",
        "\n",
        "By leveraging classification algorithms, such models can assist in quality control, wine authentication, and recommendation systems. The dataset will first be explored and analyzed, followed by preprocessing steps such as normalization to ensure that all features contribute equally. The performance of different classifiers will be evaluated to determine the most effective model for accurate wine classification."
      ],
      "metadata": {
        "id": "CNfbHwCaF3PV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykUliMQiAJsr"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "The first cell below loads the ``wine dataset``.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
        "\n",
        "The second cell prints out the description of this dataset. In short, it is for **classification of wines** based on features like alcohol, magnesium and colour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:39.776859Z",
          "start_time": "2022-11-24T17:10:38.234261Z"
        },
        "id": "bnk3wTy3AJsr"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = load_wine()\n",
        "\n",
        "# Get the X (feature matrix) and y (class label vector) from the data\n",
        "X, y = dataset.data, dataset.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation:**\n",
        "\n",
        "This code is using Scikit-learn's load_wine function to load the Wine dataset, which is a well-known dataset used for classification tasks. Below is a step-by-step explanation:\n",
        "\n",
        "**Importing the Dataset**\n",
        "\n",
        "    from sklearn.datasets import load_wine\n",
        "\n",
        "a) This imports the load_wine function from the sklearn.datasets module.\n",
        "\n",
        "b) load_wine() is a function that loads the Wine dataset, which contains chemical properties of different wines and their corresponding labels.\n",
        "\n",
        "**Loading the Dataset**\n",
        "\n",
        "    dataset = load_wine()\n",
        "\n",
        "load_wine() returns a dictionary-like object (a Bunch object) containing the dataset.\n",
        "\n",
        "This dataset includes:\n",
        "\n",
        "a) data: The feature matrix (chemical composition of wine samples).\n",
        "\n",
        "b) target: The class labels (wine types).\n",
        "\n",
        "c) feature_names: The names of the features.\n",
        "\n",
        "d) target_names: The names of the classes.\n",
        "\n",
        "3. Extracting Features and Labels\n",
        "\n",
        "X, y = dataset.data, dataset.target\n",
        "\n",
        "    X = dataset.data\n",
        "\n",
        "a) X (feature matrix) contains the numerical values of the chemical composition of wines.\n",
        "\n",
        "b) Each row represents a different wine sample.\n",
        "\n",
        "c) Each column represents a different feature (like alcohol content, malic acid, etc.).\n",
        "\n",
        "    y = dataset.target\n",
        "\n",
        "a) y is the target vector (class labels) that indicates the type of wine.\n",
        "\n",
        "b) The dataset has three different classes (0, 1, 2), which correspond to different types of wine.\n"
      ],
      "metadata": {
        "id": "Z5OeyRPSBgwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m7tUeoziAJss",
        "outputId": "4fdfd986-4cb2-476f-eaba-4ca0229e3f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 178\n",
            ":Number of Attributes: 13 numeric, predictive attributes and the class\n",
            ":Attribute Information:\n",
            "    - Alcohol\n",
            "    - Malic acid\n",
            "    - Ash\n",
            "    - Alcalinity of ash\n",
            "    - Magnesium\n",
            "    - Total phenols\n",
            "    - Flavanoids\n",
            "    - Nonflavanoid phenols\n",
            "    - Proanthocyanins\n",
            "    - Color intensity\n",
            "    - Hue\n",
            "    - OD280/OD315 of diluted wines\n",
            "    - Proline\n",
            "    - class:\n",
            "        - class_0\n",
            "        - class_1\n",
            "        - class_2\n",
            "\n",
            ":Summary Statistics:\n",
            "\n",
            "============================= ==== ===== ======= =====\n",
            "                                Min   Max   Mean     SD\n",
            "============================= ==== ===== ======= =====\n",
            "Alcohol:                      11.0  14.8    13.0   0.8\n",
            "Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "Ash:                          1.36  3.23    2.36  0.27\n",
            "Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "Magnesium:                    70.0 162.0    99.7  14.3\n",
            "Total Phenols:                0.98  3.88    2.29  0.63\n",
            "Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "Hue:                          0.48  1.71    0.96  0.23\n",
            "OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "Proline:                       278  1680     746   315\n",
            "============================= ==== ===== ======= =====\n",
            "\n",
            ":Missing Attribute Values: None\n",
            ":Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            ":Creator: R.A. Fisher\n",
            ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            ":Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners:\n",
            "\n",
            "Forina, M. et al, PARVUS -\n",
            "An Extendible Package for Data Exploration, Classification and Correlation.\n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science.\n",
            "\n",
            ".. dropdown:: References\n",
            "\n",
            "    (1) S. Aeberhard, D. Coomans and O. de Vel,\n",
            "    Comparison of Classifiers in High Dimensional Settings,\n",
            "    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
            "    Mathematics and Statistics, James Cook University of North Queensland.\n",
            "    (Also submitted to Technometrics).\n",
            "\n",
            "    The data was used with many others for comparing various\n",
            "    classifiers. The classes are separable, though only RDA\n",
            "    has achieved 100% correct classification.\n",
            "    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
            "    (All results using the leave-one-out technique)\n",
            "\n",
            "    (2) S. Aeberhard, D. Coomans and O. de Vel,\n",
            "    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
            "    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
            "    Mathematics and Statistics, James Cook University of North Queensland.\n",
            "    (Also submitted to Journal of Chemometrics).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out the dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanantion:**\n",
        "The command print(dataset.DESCR) prints a detailed description of the Wine dataset. This description provides an overview of the dataset, including its origin, purpose, structure, and features. The dataset is sourced from the UCI Machine Learning Repository and is used for classification tasks. It consists of 178 wine samples, each described by 13 numerical features representing chemical properties such as alcohol content, malic acid, ash, alkalinity, magnesium, and phenols. The dataset is labeled into three classes (0, 1, and 2), corresponding to three different types of wine grown in the same region in Italy. The description also includes details about the feature names, class distribution, and references to original research papers where the dataset was used. This information helps researchers and data scientists understand the dataset before applying machine learning models."
      ],
      "metadata": {
        "id": "1pcXWLz4Cbz9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8-VKweRAJst"
      },
      "source": [
        "## Scaling datasets\n",
        "\n",
        "**TODO**: do the normalisation and scaling as per the lab notes to create 2 additional versions of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:41.282576Z",
          "start_time": "2022-11-24T17:10:41.278257Z"
        },
        "id": "xiOMGvD2AJst"
      },
      "outputs": [],
      "source": [
        "# Normalising feature matrix\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "normalizer.fit(X)\n",
        "X_normalised = normalizer.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation**\n",
        "\n",
        "This code snippet is used to normalize the feature matrix X using L2 normalization, which scales each sample (row) to have a unit norm (i.e., the sum of the squared values of each row equals 1). The goal of normalization is to ensure that all features contribute equally to a machine learning model by removing scale differences.\n",
        "\n",
        "**Importing the Normalizer**\n",
        "\n",
        "    from sklearn.preprocessing import Normalizer\n",
        "\n",
        "This imports the Normalizer class from Scikit-learn’s preprocessing module.\n",
        "\n",
        "The Normalizer applies row-wise normalization, meaning each wine sample (row) is transformed individually.\n",
        "\n",
        "Creating a Normalizer Instance\n",
        "\n",
        "    normalizer = Normalizer()\n",
        "This creates an instance of the Normalizer class.\n",
        "\n",
        "By default, Normalizer() applies L2 normalization, which scales each row so that the sum of squared values equals 1.\n",
        "\n",
        "Fitting the Normalizer to the Data\n",
        "\n",
        "    normalizer.fit(X)\n",
        "\n",
        "The fit(X) step computes necessary statistics for normalization, but in the case of Normalizer, this step is not strictly necessary because it does not learn parameters like mean and variance (as in standardization).\n",
        "\n",
        "However, it is included as a convention.\n",
        "\n",
        "Transforming the Feature Matrix\n",
        "\n",
        "    X_normalised = normalizer.transform(X)\n",
        "\n",
        "The transform(X) method applies L2 normalization to each row.\n",
        "\n",
        "The transformed feature matrix X_normalised contains scaled values where the sum of squared feature values in each row equals 1.\n",
        "\n",
        "This transformation ensures that features with larger numerical ranges do not dominate over others.\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "Normalization is different from standardization (which centers data to have zero mean and unit variance).\n",
        "\n",
        "Normalizer is useful in models where the magnitude of features is important, such as in K-Nearest Neighbors (KNN) or Neural Networks.\n",
        "\n",
        "It ensures that all samples (rows) contribute equally to the learning process by rescaling them to have the same norm.\n"
      ],
      "metadata": {
        "id": "7HQW83DLDWqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:41.297659Z",
          "start_time": "2022-11-24T17:10:41.285121Z"
        },
        "id": "ebjb0N25AJst"
      },
      "outputs": [],
      "source": [
        "# TODO: Scaling feature matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9q51L2IAJst"
      },
      "source": [
        "## MLP validation\n",
        "\n",
        "The 1st cell below does the following:\n",
        "\n",
        " * Splits the dataset for training and testing (using the original feature values ``X``)\n",
        " * Creates an 3-layer ``MLPClassifier`` with 10 neurons in the hidden layer, to be trained for 100 epocs (iterations)\n",
        " * Trains the MLP and tests it\n",
        " * Calculates and prints out a confusion matrix and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKjPjAfGAJst"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn import metrics\n",
        "\n",
        "def print_results(scores):\n",
        "    print(\"Accuracy:          %0.2f (+/- %0.2f)\" % (scores['test_score'].mean(), scores['test_score'].std() * 2))\n",
        "    print(\"Training time (s): %0.2f (+/- %0.2f)\" % (scores['fit_time'].mean(), scores['fit_time'].std() * 2))\n",
        "    print(\"Testing time (s):  %0.2f (+/- %0.2f)\" % (scores['score_time'].mean(), scores['score_time'].std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJS76Q1AAJsu"
      },
      "outputs": [],
      "source": [
        "# Instantiating MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(10), max_iter=10)\n",
        "\n",
        "# Validating MLP model\n",
        "scores = cross_validate(model, X, y, cv=5)\n",
        "\n",
        "# Printing performance results\n",
        "print_results(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlG3cJKsAJsu"
      },
      "source": [
        "You should have seen warnings about the MLP not having converged above, and a rather sub-optimal performance!\n",
        "\n",
        "**TODO**: see what the performance is like by just increasing the number of training iterations (epochs).\n",
        "\n",
        "**QUESTION**: after increasing ``max_iter`` to the point the convergance warning disappears, what's the performance like? Is it good enough?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceR0QD6wAJsu"
      },
      "source": [
        "## Normalised vs Scaled feature values\n",
        "\n",
        "If you tweak the number of neurons in the hidden layer and the maximum number of iterations (and other hyper-parameters), you will probably find that the performance remains quite poor.\n",
        "\n",
        "So, let us now move on to comparing the performance when using the normalised and scaled feature matrices instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duE-Qg3vAJsu"
      },
      "source": [
        "### Normalised feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:48.052080Z",
          "start_time": "2022-11-24T17:10:42.201072Z"
        },
        "id": "tgP_h4E4AJsv"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJYImBzAJsv"
      },
      "source": [
        "### Scaled feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:48.916816Z",
          "start_time": "2022-11-24T17:10:48.054587Z"
        },
        "id": "uKhXzz0VAJsv"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYS1uT4AJsv"
      },
      "source": [
        "## Discussion and Conclusions\n",
        "\n",
        "Above, you should be able to make a few key observations regarding:\n",
        "\n",
        "* The performance of MLPs in general on this dataset\n",
        "* How the performance is affected by\n",
        "  - Hyper-parameters like the number of neurons and number of epochs\n",
        "  - Data processing: original feature values vs normalised vs scaled\n",
        "\n",
        "What seems to be best?\n",
        "\n",
        "What seems to be worst?\n",
        "\n",
        "What seems to make the biggest difference to the performance?\n",
        "\n",
        "**PS**: Feel free to play around with other hyper-parameters as well, which you can see in the API reference documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}